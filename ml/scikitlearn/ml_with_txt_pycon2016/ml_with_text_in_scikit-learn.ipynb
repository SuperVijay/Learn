{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "(150, 4)\n",
      "(150,)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "[2]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "# Note that x is 2 dimensional and y is one dimensional\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "\n",
    "print (iris.feature_names)\n",
    "print (x.shape)\n",
    "print (y.shape)\n",
    "# classification problem (not regression)\n",
    "# in order to build a model, the featuers must be numerica values\n",
    "# every observation must have same features in same order..esp loadin from text.\n",
    "print (y)\n",
    "\n",
    "pd.DataFrame(x, columns=iris.feature_names).head()\n",
    "\n",
    "# instantiate the model with default params\n",
    "# algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "#           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
    "#           weights='uniform'\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# fit the model with data (in-place)\n",
    "# learning the relation between the featuers and response (x and y)\n",
    "knn.fit(x,y)\n",
    "\n",
    "# predict the response for a new observation\n",
    "predict = knn.predict([[1,5,4, 9]])\n",
    "print (predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "['cab', 'call', 'me', 'please', 'tonight', 'you']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    0     1   1       1        0    0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_train = ['call you tonight', 'Call me a cab', 'PLEASE call me ..please']\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "\n",
    "# learn the vocabular of the training data\n",
    "vect.fit(simple_train)\n",
    "print (vect)\n",
    "\n",
    "# examine fitted vocabulary\n",
    "print (vect.get_feature_names() )\n",
    "\n",
    "\n",
    "simple_train_dtm = vect.transform(simple_train)\n",
    "simple_train_dtm.toarray()\n",
    "\n",
    "# numpy doesn't support sparse representation. Scipy does.\n",
    "type(simple_train_dtm)\n",
    "\n",
    "pd.DataFrame(simple_train_dtm.toarray(), columns=vect.get_feature_names())\n",
    "\n",
    "simple_test = [\"please don't call me\"]\n",
    "simple_test_dtm = vect.transform(simple_test)\n",
    "simple_test_dtm.toarray()\n",
    "pd.DataFrame(simple_test_dtm.toarray(), columns=vect.get_feature_names())\n",
    "# note that the DONT is not detected because the model never got trained on that.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Summary: </h3>\n",
    "<ul>\n",
    " <li>\n",
    " vect.fit(train) <b> learns the vocabulary </b> of the training data\n",
    " </li>\n",
    " \n",
    "<li>  vect.transform(train) use the <b> fitted vocab </b> to bulid a matrix</li>\n",
    "  <li>vect.transform(test) uses the <b> fitted vocab </b> to build matrix from the test data and ignores tokens it hasn't see before.</li>\n",
    "</ul>\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 2)\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: label, dtype: int64\n",
      "(4179,)\n",
      "(1393,)\n",
      "(4179,)\n",
      "(1393,)\n",
      "Wall time: 1 ms\n",
      "98.8513998564\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.98664310005369615"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = 'data/sms.tsv'\n",
    "#url = 'https://raw.githubusercontent.com/justmarkham/pycon-2016-tutorial/master/data/sms.tsv'\n",
    "sms = pd.read_table(path, header=None, names=['label', 'message'])\n",
    "print(sms.shape)\n",
    "#sms.head(10)\n",
    "print (sms.label.value_counts())\n",
    "\n",
    "#convert label to a numerical value\n",
    "sms['label_num']= sms.label.map({'ham':0, 'spam':1}) \n",
    "sms.head(10)\n",
    "\n",
    "# note that x is 1 dimensional and y is also 1 dimensional\n",
    "# we should be keeping x as 1-d because the count verctorizer \n",
    "#  is going to transform this to 2 dimensional.\n",
    "x=sms.message\n",
    "y=sms.label_num\n",
    "\n",
    "# split x and y inot training and test datasets\n",
    "# depricated cross_validation.. move to model_selection\n",
    "\n",
    "# Important: Do the train test split before vectorization.\n",
    "#   our premise is that past data is predictive of future\n",
    "#   if you vectorize before you test, you are going to get every\n",
    "#   token in the entire dataset captured in the features and \n",
    "#   it is not true representation of real-life secenario.\n",
    "\n",
    "from sklearn.cross_validation import train_test_split as tts\n",
    "x_train, x_test, y_train, y_test = tts(x, y, random_state = 1)\n",
    "print (x_train.shape)\n",
    "print (x_test.shape)\n",
    "print (y_train.shape)\n",
    "print (y_test.shape)\n",
    "\n",
    "# instantiate the vectorizer\n",
    "vet = CountVectorizer()\n",
    "# Learn the training data vocabular\n",
    "#vect.fit(x_train)\n",
    "#x_train_dtm = vect.transform(x_train)\n",
    "# equivalent\n",
    "x_train_dtm = vect.fit_transform(x_train)\n",
    "#x_train_dtm\n",
    "\n",
    "x_test_dtm = vect.transform(x_test)\n",
    "#x_test_dtm\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# measure the time for training the dataset\n",
    "%time nb.fit(x_train_dtm, y_train)\n",
    "\n",
    "#make class predictions for x_test_dtm\n",
    "y_pred_class = nb.predict(x_test_dtm)\n",
    "\n",
    "# calculate accuracy of class predictions\n",
    "from sklearn import metrics\n",
    "accuarcy_score = metrics.accuracy_score(y_test, y_pred_class)\n",
    "print (100*accuarcy_score)\n",
    "# print the confusion matrix  1 positive 0 negative\n",
    "# [ true_negatives false_positives\n",
    "#   false_negatives true_positives]\n",
    "metrics.confusion_matrix(y_test, y_pred_class)\n",
    "\n",
    "\n",
    "# print message text for the false positives (ham incorrectly classified)\n",
    "x_test[ (y_pred_class==1) & (y_test==0)]\n",
    "\n",
    "# print message text for the false negatives (spam incorrectly classified)\n",
    "x_test[y_pred_class < y_test]\n",
    "\n",
    "# example of false negative\n",
    "x_test[3132]\n",
    "\n",
    "# calculate predicated probabilites for x_test_dtm (poor)\n",
    "y_pred_prob = nb.predict_proba(x_test_dtm)[:,1]\n",
    "y_pred_prob\n",
    "\n",
    "#calculate AUC\n",
    "metrics.roc_auc_score(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 50.9 ms\n",
      "[ 0.01269556  0.00347183  0.00616517 ...,  0.03354907  0.99725053\n",
      "  0.00157706]\n",
      "0.987796123475\n",
      "0.993681761231\n"
     ]
    }
   ],
   "source": [
    "# import and instantiate a logistics regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "%time logreg.fit(x_train_dtm, y_train)\n",
    "\n",
    "y_pred_class = logreg.predict(x_test_dtm)\n",
    "y_pred_prob = logreg.predict_proba(x_test_dtm)[:,1]\n",
    "print (y_pred_prob)\n",
    "accuracy_score=metrics.accuracy_score(y_test, y_pred_class)\n",
    "print (accuracy_score)\n",
    "auc_score=metrics.roc_auc_score(y_test, y_pred_prob)\n",
    "print (auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
